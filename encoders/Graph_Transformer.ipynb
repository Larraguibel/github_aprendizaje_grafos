{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías"
      ],
      "metadata": {
        "id": "eVAjApOxCRxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gswXzSFAM2AE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d8a9ba-a52f-4167-99cf-e54d28a95317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-fHb8-5eM8y-"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Batch, Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool, TransformerConv\n",
        "from torch_geometric.utils import from_networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura desde Drive"
      ],
      "metadata": {
        "id": "KTk5cP4dCQeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruNxl3LnQmHp",
        "outputId": "858ebca5-e28f-4bca-cc11-26420111fa0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zLjqv-k8Qp-d"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/grafo_santiago_filtrado_con_embeddings.gexf\"\n",
        "Gx = nx.read_gexf(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Transformer"
      ],
      "metadata": {
        "id": "hBXfJf3ECO3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Xmm-IKTNEAw"
      },
      "outputs": [],
      "source": [
        "class GraphormerEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=128, num_layers=4, num_heads=4, out_dim=128):\n",
        "        super().__init__()\n",
        "        # Proyección lineal del embedding de Alpha Earth\n",
        "        self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
        "\n",
        "        # Capas del Graphormer\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(\n",
        "                TransformerConv(\n",
        "                    in_channels=hidden_dim,\n",
        "                    out_channels=hidden_dim,\n",
        "                    heads=num_heads,\n",
        "                    concat=False,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Normalizaciones por capa (una LayerNorm por cada TransformerConv)\n",
        "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        # Proyección final del embedding del grafo al espacio de salida (embedding de ciudad)\n",
        "        self.out_proj = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        \"\"\"\n",
        "        x: [N, in_dim]     - features de nodos\n",
        "        edge_index: [2, E] - aristas del grafo en formato COO\n",
        "        batch: [N]         - asigna cada nodo a un grafo (aquí, subgrafos / ciudades)\n",
        "        \"\"\"\n",
        "        # Pasamos las features de los nodos al espacio oculto\n",
        "        h = self.input_proj(x) # [N, hidden_dim]\n",
        "\n",
        "        # Aplicamos num_layers veces: TransformerConv + residual + LayerNorm + ReLU\n",
        "        for conv, norm in zip(self.layers, self.norms):\n",
        "            h_res = h                      # conexión residual\n",
        "            h = conv(h, edge_index)        # mensaje + atención entre nodos\n",
        "            h = norm(h + h_res)            # residual + normalización\n",
        "            h = F.relu(h)                  # no linealidad\n",
        "\n",
        "        # Readout: agregamos todos los nodos de cada grafo en un solo vector (mean pooling)\n",
        "        g = global_mean_pool(h, batch)\n",
        "\n",
        "        # Proyección al espacio de embedding final\n",
        "        g = self.out_proj(g)\n",
        "\n",
        "        # Normalizamos para que los embeddings queden en la esfera unitaria (útil para contraste)\n",
        "        g = F.normalize(g, p=2, dim=-1)\n",
        "        return g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentaciones"
      ],
      "metadata": {
        "id": "kvv_6YHQCMsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hIGWJLnyNJI7"
      },
      "outputs": [],
      "source": [
        "# Devuelve un grafo que tiene drop_prob de sus aristas enmascaradas\n",
        "def random_edge_dropout(data, drop_prob=0.2):\n",
        "    edge_index = data.edge_index\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    mask = torch.rand(num_edges, device=edge_index.device) > drop_prob\n",
        "    new_edge_index = edge_index[:, mask]\n",
        "\n",
        "    new_data = data.clone()\n",
        "    new_data.edge_index = new_edge_index\n",
        "    return new_data\n",
        "\n",
        "# Devuelve un grafo que tiene mask_prob de sus features enmascaradas\n",
        "def random_feature_masking(data, mask_prob=0.2):\n",
        "    x = data.x.clone()\n",
        "    mask = torch.rand_like(x) < mask_prob\n",
        "    x[mask] = 0.0\n",
        "\n",
        "    new_data = data.clone()\n",
        "    new_data.x = x\n",
        "    return new_data\n",
        "\n",
        "# Aplica las dos transformaciones anteriores.\n",
        "def graphcl_augment(data):\n",
        "    v1 = random_edge_dropout(data, drop_prob=0.2)\n",
        "    v1 = random_feature_masking(v1, mask_prob=0.2)\n",
        "\n",
        "    v2 = random_edge_dropout(data, drop_prob=0.2)\n",
        "    v2 = random_feature_masking(v2, mask_prob=0.2)\n",
        "\n",
        "    return v1, v2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Loss"
      ],
      "metadata": {
        "id": "7PbWb8BGCJGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I-oGXAk9NKZq"
      },
      "outputs": [],
      "source": [
        "def nt_xent_loss(z1, z2, temperature=0.2):\n",
        "    # Número de grafos en z1 (mismos que z2)\n",
        "    batch_size = z1.size(0)\n",
        "\n",
        "    # Concatenamos las 2 augmentaciones\n",
        "    z = torch.cat([z1, z2], dim=0)\n",
        "\n",
        "    # Similitud mediante producto punto\n",
        "    sim = torch.matmul(z, z.t())\n",
        "\n",
        "    # Escalamos con temperatura. Temperaturas más bajas hacen el contraste más agresivo\n",
        "    sim = sim / temperature\n",
        "\n",
        "    # Eliminamos la diagonal\n",
        "    mask = torch.eye(2 * batch_size, device=z.device, dtype=torch.bool)\n",
        "    sim = sim.masked_fill(mask, -1e9)\n",
        "\n",
        "    # Construcción de labels\n",
        "    # Las vistas augmentadas son vistas como pares positivos (i + batch_size)\n",
        "    labels = torch.arange(2 * batch_size, device=z.device)\n",
        "    labels = (labels + batch_size) % (2 * batch_size)\n",
        "\n",
        "    # Cross-entropy\n",
        "    loss = F.cross_entropy(sim, labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento del dataset y obtención de subgrafos"
      ],
      "metadata": {
        "id": "Q6iCDwTrCGUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB7A2DWmMkS1",
        "outputId": "e2bd1a9e-e8ec-4a66-c656-4d277481de35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subgrafos creados: 46\n"
          ]
        }
      ],
      "source": [
        "# Se convierte al formato de torch_geometric\n",
        "data_raw_full = from_networkx(Gx)\n",
        "\n",
        "# Keys de los embeddings\n",
        "A_keys = [f\"A{i:02d}\" for i in range(64)]\n",
        "\n",
        "# Matriz de embeddings\n",
        "feat_tensors_full = []\n",
        "for k in A_keys:\n",
        "    t = getattr(data_raw_full, k).view(-1, 1).float()\n",
        "    feat_tensors_full.append(t)\n",
        "\n",
        "# x_full tendrá shape [num_nodes, 64] con los embeddings AlphaEarth concatenados\n",
        "x_full = torch.cat(feat_tensors_full, dim=1)\n",
        "\n",
        "# Armamos el objeto Data de PyG para el grafo completo de Santiago\n",
        "data_santiago = Data(\n",
        "    x=x_full,\n",
        "    edge_index=data_raw_full.edge_index,\n",
        "    num_nodes=data_raw_full.num_nodes\n",
        ")\n",
        "\n",
        "# Extraemos los IDs de nodos y sus coordenadas geográficas desde el grafo de NetworkX\n",
        "node_ids = list(Gx.nodes())\n",
        "lats = np.array([Gx.nodes[n]['lat'] for n in node_ids])\n",
        "lons = np.array([Gx.nodes[n]['lon'] for n in node_ids])\n",
        "\n",
        "# Definimos la grilla en latitud/longitud:\n",
        "num_lat_bins = 8\n",
        "num_lon_bins = 8\n",
        "min_nodes = 50   # para saltar subgrafos muy chicos\n",
        "\n",
        "# Bordes de las celdas de la grilla\n",
        "lat_bins = np.linspace(lats.min(), lats.max(), num_lat_bins + 1)\n",
        "lon_bins = np.linspace(lons.min(), lons.max(), num_lon_bins + 1)\n",
        "\n",
        "subgraphs_pyg = []\n",
        "\n",
        "def build_data_from_nx(G_sub):\n",
        "    \"\"\"\n",
        "    Convierte un subgrafo de NetworkX en un objeto Data de PyG,\n",
        "    usando únicamente las 64 dimensiones A00..A63 como features de nodo.\n",
        "    \"\"\"\n",
        "    data_raw = from_networkx(G_sub)\n",
        "\n",
        "    feat_tensors = []\n",
        "    for k in A_keys:\n",
        "        t = getattr(data_raw, k).view(-1, 1).float()\n",
        "        feat_tensors.append(t)\n",
        "    x = torch.cat(feat_tensors, dim=1)\n",
        "\n",
        "    data = Data(\n",
        "        x=x,\n",
        "        edge_index=data_raw.edge_index,\n",
        "        num_nodes=data_raw.num_nodes\n",
        "    )\n",
        "    return data\n",
        "\n",
        "for i in range(num_lat_bins):\n",
        "    for j in range(num_lon_bins):\n",
        "        mask = (\n",
        "            (lats >= lat_bins[i]) & (lats < lat_bins[i + 1]) &\n",
        "            (lons >= lon_bins[j]) & (lons < lon_bins[j + 1])\n",
        "        )\n",
        "        idxs = np.where(mask)[0]\n",
        "        if len(idxs) < min_nodes:\n",
        "            continue\n",
        "\n",
        "        nodes_bin = [node_ids[k] for k in idxs]\n",
        "        G_sub = Gx.subgraph(nodes_bin).copy()\n",
        "        if G_sub.number_of_edges() == 0:\n",
        "            continue\n",
        "\n",
        "        data_sub = build_data_from_nx(G_sub)\n",
        "        subgraphs_pyg.append(data_sub)\n",
        "\n",
        "print(\"Subgrafos creados:\", len(subgraphs_pyg))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del Dataset (pytorch)"
      ],
      "metadata": {
        "id": "4SEWkuMMCDCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZ3cP7mgNS7g"
      },
      "outputs": [],
      "source": [
        "class CityGraphDataset(Dataset):\n",
        "    def __init__(self, graphs):\n",
        "        super().__init__()\n",
        "        self.graphs = graphs\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "dataset = CityGraphDataset(subgraphs_pyg)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bucle de entrenamiento"
      ],
      "metadata": {
        "id": "zuoHgQOqCB9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmGz24U7NUdC",
        "outputId": "7a25b242-f778-497f-e049-5815235234c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Loss: 1.9272\n",
            "Epoch 002 | Loss: 0.9886\n",
            "Epoch 003 | Loss: 0.9316\n",
            "Epoch 004 | Loss: 1.0652\n",
            "Epoch 005 | Loss: 0.9961\n",
            "Epoch 006 | Loss: 0.7809\n",
            "Epoch 007 | Loss: 0.8608\n",
            "Epoch 008 | Loss: 0.8196\n",
            "Epoch 009 | Loss: 0.8723\n",
            "Epoch 010 | Loss: 0.6315\n",
            "Epoch 011 | Loss: 0.7257\n",
            "Epoch 012 | Loss: 0.7251\n",
            "Epoch 013 | Loss: 0.7509\n",
            "Epoch 014 | Loss: 0.6153\n",
            "Epoch 015 | Loss: 0.6322\n",
            "Epoch 016 | Loss: 0.3792\n",
            "Epoch 017 | Loss: 0.6779\n",
            "Epoch 018 | Loss: 0.8287\n",
            "Epoch 019 | Loss: 0.6364\n",
            "Epoch 020 | Loss: 0.5621\n",
            "Epoch 021 | Loss: 0.5944\n",
            "Epoch 022 | Loss: 0.6309\n",
            "Epoch 023 | Loss: 0.4832\n",
            "Epoch 024 | Loss: 0.5493\n",
            "Epoch 025 | Loss: 0.5223\n",
            "Epoch 026 | Loss: 0.4246\n",
            "Epoch 027 | Loss: 0.6704\n",
            "Epoch 028 | Loss: 0.4387\n",
            "Epoch 029 | Loss: 0.4975\n",
            "Epoch 030 | Loss: 0.4613\n",
            "Epoch 031 | Loss: 0.3391\n",
            "Epoch 032 | Loss: 0.5603\n",
            "Epoch 033 | Loss: 0.4406\n",
            "Epoch 034 | Loss: 0.4940\n",
            "Epoch 035 | Loss: 0.5353\n",
            "Epoch 036 | Loss: 0.4311\n",
            "Epoch 037 | Loss: 0.3060\n",
            "Epoch 038 | Loss: 0.3617\n",
            "Epoch 039 | Loss: 0.3930\n",
            "Epoch 040 | Loss: 0.3971\n",
            "Epoch 041 | Loss: 0.3760\n",
            "Epoch 042 | Loss: 0.4136\n",
            "Epoch 043 | Loss: 0.3215\n",
            "Epoch 044 | Loss: 0.5618\n",
            "Epoch 045 | Loss: 0.5169\n",
            "Epoch 046 | Loss: 0.3260\n",
            "Epoch 047 | Loss: 0.3505\n",
            "Epoch 048 | Loss: 0.3481\n",
            "Epoch 049 | Loss: 0.4622\n",
            "Epoch 050 | Loss: 0.2782\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dimension del embedding del nodo (64)\n",
        "in_dim = dataset[0].x.size(1)\n",
        "model = GraphormerEncoder(in_dim=in_dim, hidden_dim=128, num_layers=4, num_heads=4, out_dim=128).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_graphs = 0\n",
        "\n",
        "    for batch_data in loader:\n",
        "        batch_data = batch_data.to(device)\n",
        "\n",
        "        # Separar en lista de grafos individuales\n",
        "        data_list = batch_data.to_data_list()\n",
        "\n",
        "        # Se crean las augmentaciones\n",
        "        v1_list, v2_list = [], []\n",
        "        for g in data_list:\n",
        "            a1, a2 = graphcl_augment(g)\n",
        "            v1_list.append(a1)\n",
        "            v2_list.append(a2)\n",
        "\n",
        "        # Se modelan como batches independientes\n",
        "        v1_batch = Batch.from_data_list(v1_list).to(device)\n",
        "        v2_batch = Batch.from_data_list(v2_list).to(device)\n",
        "\n",
        "        # Se obtienen sus embeddings\n",
        "        z1 = model(v1_batch.x, v1_batch.edge_index, v1_batch.batch)\n",
        "        z2 = model(v2_batch.x, v2_batch.edge_index, v2_batch.batch)\n",
        "\n",
        "        # Se cálcula la pérdida\n",
        "        loss = nt_xent_loss(z1, z2, temperature=0.2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * z1.size(0)\n",
        "        total_graphs += z1.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_graphs\n",
        "    print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sacamos el embedding de Santiago"
      ],
      "metadata": {
        "id": "WGbxYHoWBxyK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXNhs4oANV7E",
        "outputId": "4ffed676-a3b9-4bc7-e8a5-2f39a2974da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding de Santiago: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "data_santiago = data_santiago.to(device)\n",
        "batch_full = torch.zeros(data_santiago.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    emb_santiago = model(data_santiago.x, data_santiago.edge_index, batch_full)  # [1, out_dim]\n",
        "    emb_santiago = emb_santiago[0]\n",
        "    print(\"Embedding de Santiago:\", emb_santiago.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificar que acerca augmentaciones y separa subgrafos distintos"
      ],
      "metadata": {
        "id": "HvIPvVvOB9hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subgrafos al azar"
      ],
      "metadata": {
        "id": "DN-N8repDi0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Tomamos un subgrafo al azar\n",
        "g = random.choice(subgraphs_pyg)\n",
        "g = g.to(device)\n",
        "\n",
        "# Sacamos embedding del subgrafo original\n",
        "b_orig = Batch.from_data_list([g]).to(device)\n",
        "with torch.no_grad():\n",
        "    z_orig = model(b_orig.x, b_orig.edge_index, b_orig.batch)[0]\n",
        "\n",
        "# Augmentamos el grafo\n",
        "a1, a2 = graphcl_augment(g)\n",
        "\n",
        "b1 = Batch.from_data_list([a1]).to(device)\n",
        "b2 = Batch.from_data_list([a2]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z1 = model(b1.x, b1.edge_index, b1.batch)[0]\n",
        "    z2 = model(b2.x, b2.edge_index, b2.batch)[0]\n",
        "\n",
        "# Cosenos entre original y augmentaciones\n",
        "cos_orig_a1 = F.cosine_similarity(z_orig, z1, dim=0).item()\n",
        "cos_orig_a2 = F.cosine_similarity(z_orig, z2, dim=0).item()\n",
        "cos_a1_a2   = F.cosine_similarity(z1, z2, dim=0).item()\n",
        "\n",
        "print(\"cosine(original, vista1) =\", cos_orig_a1)\n",
        "print(\"cosine(original, vista2) =\", cos_orig_a2)\n",
        "print(\"cosine(vista1, vista2)   =\", cos_a1_a2)\n",
        "\n",
        "# Sacamos otro grafo al azar\n",
        "g2 = random.choice([h for h in subgraphs_pyg if h is not g]).to(device)\n",
        "b3 = Batch.from_data_list([g2]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z3 = model(b3.x, b3.edge_index, b3.batch)[0]\n",
        "\n",
        "# Comparamos su similaridad\n",
        "cos_diff = F.cosine_similarity(z_orig, z3, dim=0).item()\n",
        "print(\"cosine(original, otro subgrafo) =\", cos_diff)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yv4D_yrCWjR",
        "outputId": "c798ad6a-9f67-4771-a05a-994f4d3c5cf4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine(original, vista1) = 0.9545512199401855\n",
            "cosine(original, vista2) = 0.9691970348358154\n",
            "cosine(vista1, vista2)   = 0.995003342628479\n",
            "cosine(original, otro subgrafo) = -0.06205920875072479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grafo Santiago"
      ],
      "metadata": {
        "id": "hNx-kSB7DfaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# grafo completo\n",
        "g = data_santiago.to(device)\n",
        "\n",
        "# augmentaciones del grafo completo\n",
        "a1, a2 = graphcl_augment(g)\n",
        "\n",
        "b_orig = Batch.from_data_list([g]).to(device)\n",
        "b1     = Batch.from_data_list([a1]).to(device)\n",
        "b2     = Batch.from_data_list([a2]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z_orig = model(b_orig.x, b_orig.edge_index, b_orig.batch)[0]\n",
        "    z1     = model(b1.x, b1.edge_index, b1.batch)[0]\n",
        "    z2     = model(b2.x, b2.edge_index, b2.batch)[0]\n",
        "\n",
        "print(\"cos(original, vista1) =\", F.cosine_similarity(z_orig, z1, dim=0).item())\n",
        "print(\"cos(original, vista2) =\", F.cosine_similarity(z_orig, z2, dim=0).item())\n",
        "print(\"cos(vista1, vista2)   =\", F.cosine_similarity(z1, z2, dim=0).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO600EBmC0MW",
        "outputId": "ed13c66e-b6ad-4a82-bef8-27732721978c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cos(original, vista1) = 0.9369595646858215\n",
            "cos(original, vista2) = 0.9363678693771362\n",
            "cos(vista1, vista2)   = 0.9998849630355835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = random.choice(subgraphs_pyg).to(device)\n",
        "b = Batch.from_data_list([g]).to(device)\n",
        "with torch.no_grad():\n",
        "    z_sub = model(b.x, b.edge_index, b.batch)[0]\n",
        "\n",
        "cos_orig_sub = F.cosine_similarity(z_orig, z_sub, dim=0).item()\n",
        "print(\"cos(Santiago completo, subgrafo al azar) =\", cos_orig_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh6SNz5nC1uY",
        "outputId": "269a67ae-55e2-4830-aa26-16e560891c9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cos(Santiago completo, subgrafo al azar) = 0.08978444337844849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_JoSh09DWXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}